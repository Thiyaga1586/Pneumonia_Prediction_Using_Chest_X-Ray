{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd3dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db5903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score,accuracy_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33053755",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomApply([transforms.ColorJitter(brightness=0.1, contrast=0.1)], p=0.3),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "val_transformer = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eeca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "dataset_root = \"/content/Final_dataset\" \n",
    "\n",
    "valid_exts = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "\n",
    "bad_images = []\n",
    "\n",
    "def is_valid_image(path):\n",
    "    try:\n",
    "        with Image.open(path) as img:\n",
    "            if img.size != (224, 224):\n",
    "                return False, \"Wrong size: \" + str(img.size)\n",
    "            if img.mode not in ['RGB', 'L']:\n",
    "                return False, \"Unsupported mode: \" + img.mode\n",
    "            return True, None\n",
    "    except Exception as e:\n",
    "        return False, f\"Unreadable: {str(e)}\"\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_root):\n",
    "    for file in files:\n",
    "        if os.path.splitext(file)[1].lower() in valid_exts:\n",
    "            filepath = os.path.join(root, file)\n",
    "            valid, reason = is_valid_image(filepath)\n",
    "            if not valid:\n",
    "                bad_images.append((filepath, reason))\n",
    "\n",
    "print(f\"\\nScan complete. Total bad images: {len(bad_images)}\")\n",
    "for path, reason in bad_images:\n",
    "    print(f\"{path} â€” {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979efeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder('/content/Final_dataset/train', transform=train_transformer)\n",
    "val_data   = datasets.ImageFolder('/content/Final_dataset/val', transform=val_transformer)\n",
    "test_data  = datasets.ImageFolder('/content/Final_dataset/test', transform=val_transformer)\n",
    "\n",
    "# Data Loader\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11819336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CBAM Attention Module\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ca = self.channel_attention(x) * x\n",
    "        avg_out = torch.mean(ca, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(ca, dim=1, keepdim=True)\n",
    "        sa = self.spatial_attention(torch.cat([avg_out, max_out], dim=1)) * ca\n",
    "        return sa\n",
    "\n",
    "\n",
    "# 2. Replace SEBlock with CBAM in ResidualBlock\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=False,dropout=0.2):\n",
    "        super().__init__()\n",
    "        stride = 2 if downsample else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.cbam = CBAMBlock(out_channels)  # âœ… CBAM\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if downsample or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.downsample(x)\n",
    "        out = self.silu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.cbam(out)\n",
    "        out = self.dropout(out)\n",
    "        out += identity\n",
    "        return self.silu(out)\n",
    "\n",
    "# ImprovedPneumoniaCNN with SE, SiLU, and Dropout\n",
    "class ImprovedPneumoniaCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 64, num_blocks=3, downsample=False)\n",
    "        self.layer2 = self._make_layer(64, 128, num_blocks=4, downsample=True)\n",
    "        self.layer3 = self._make_layer(128, 256, num_blocks=6, downsample=True)\n",
    "        self.layer4 = self._make_layer(256, 512, num_blocks=3, downsample=True)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.4)            \n",
    "        self.fc = nn.Linear(512, 1)               \n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, downsample):\n",
    "        layers = [ResidualBlock(in_channels, out_channels, downsample=downsample)]\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)       \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e62d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        stride = 2 if downsample else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.skip = nn.Sequential()\n",
    "        if downsample or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return F.relu(out + identity)\n",
    "\n",
    "\n",
    "class DeepResNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(DeepResNet, self).__init__()\n",
    "\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 64, num_blocks=3)\n",
    "        self.layer2 = self._make_layer(64, 128, num_blocks=4, downsample=True)\n",
    "        self.layer3 = self._make_layer(128, 256, num_blocks=6, downsample=True)\n",
    "        self.layer4 = self._make_layer(256, 512, num_blocks=6, downsample=True)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, 1)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, downsample=False):\n",
    "        layers = [ResidualBlock(in_channels, out_channels, downsample)]\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e88ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetB0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetB0, self).__init__()\n",
    "        self.backbone = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "\n",
    "        # Adjust for grayscale input (1-channel)\n",
    "        self.backbone.conv_stem = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "\n",
    "        # Adjust classifier for binary output\n",
    "        in_features = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b80098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImprovedPneumoniaCNN().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "num_epochs = 30\n",
    "patience = 5\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b73df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device='cuda'):\n",
    "    model.eval()\n",
    "    y_true, y_pred_probs, y_pred_labels = [], [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred_probs.extend(probs.cpu().numpy())\n",
    "            y_pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Flatten\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred_probs = np.array(y_pred_probs).flatten()\n",
    "    y_pred_labels = np.array(y_pred_labels).flatten()\n",
    "\n",
    "    # Metrics\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    auc = roc_auc_score(y_true, y_pred_probs)\n",
    "    f1 = f1_score(y_true, y_pred_labels)\n",
    "    acc = accuracy_score(y_true, y_pred_labels)\n",
    "    cm = confusion_matrix(y_true, y_pred_labels)\n",
    "    report = classification_report(y_true, y_pred_labels, target_names=[\"Normal\", \"Pneumonia\"])\n",
    "\n",
    "    # Print Metrics\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"Loss: {avg_loss:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "    # Visuals\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Normal\", \"Pneumonia\"], yticklabels=[\"Normal\", \"Pneumonia\"])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix\\nAccuracy: {acc:.2f} | F1: {f1:.2f}')\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_probs)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\", color='darkorange')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"auc\": auc,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": acc,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"report\": report\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf8e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize all models\n",
    "all_models = {\n",
    "    \"ImprovedPneumoniaCNN\": ImprovedPneumoniaCNN().to(device),\n",
    "    \"DeepResNet\": DeepResNet().to(device),\n",
    "    \"EfficientNetB0\": EfficientNetB0().to(device),\n",
    "}\n",
    "\n",
    "# Corresponding weight file paths \n",
    "model_weights = {\n",
    "    \"ImprovedPneumoniaCNN\": \"/content/ImprovedPneumoniaCNN.pth\",\n",
    "    \"DeepResNet\": \"/content/DeepResNet.pth\",\n",
    "    \"EfficientNetB0\": \"/content/EfficientNetB0.pth\",\n",
    "}\n",
    "\n",
    "# Store results\n",
    "model_results = {}\n",
    "\n",
    "for name, model in all_models.items():\n",
    "    print(f\"\\nðŸ” Evaluating {name}...\")\n",
    "    \n",
    "    # Load saved weights\n",
    "    model.load_state_dict(torch.load(model_weights[name], map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate\n",
    "    results = evaluate_model(model, test_loader, criterion, device=device)\n",
    "    model_results[name] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a5ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(y_true, y_probs, y_preds):\n",
    "    print(\"\\nEnsemble Evaluation:\")\n",
    "    print(f\"AUC: {roc_auc_score(y_true, y_probs):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_true, y_preds):.4f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_preds):.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_preds)\n",
    "    print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_true, y_preds, target_names=[\"Normal\", \"Pneumonia\"]))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Pneumonia\"], yticklabels=[\"Normal\", \"Pneumonia\"])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, color='darkorange', label=f\"AUC = {roc_auc_score(y_true, y_probs):.4f}\")\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve - Ensemble\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, dataloader, device='cuda'):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    y_true, y_pred_probs, y_pred_labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            # Get predictions from all models\n",
    "            probs = []\n",
    "            for model in models:\n",
    "                outputs = model(images)\n",
    "                prob = torch.sigmoid(outputs)\n",
    "                probs.append(prob)\n",
    "\n",
    "            # Average predictions across models\n",
    "            avg_prob = torch.stack(probs).mean(0)\n",
    "            preds = (avg_prob > 0.5).float()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred_probs.extend(avg_prob.cpu().numpy())\n",
    "            y_pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    return np.array(y_true).flatten(), np.array(y_pred_probs).flatten(), np.array(y_pred_labels).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate models\n",
    "models = [\n",
    "    DeepResNet().to(device),\n",
    "    ImprovedPneumoniaCNN().to(device),\n",
    "    EfficientNetB0().to(device),\n",
    "]\n",
    "\n",
    "# Load weights\n",
    "weights = [\n",
    "    \"DeepResNet.pth\",\n",
    "    \"ImprovedPneumoniaCNN.pth\",\n",
    "    \"EfficientNetB0.pth\",\n",
    "]\n",
    "\n",
    "for model, path in zip(models, weights):\n",
    "    model.load_state_dict(torch.load(path, map_location=device),strict=False)\n",
    "    model.eval()\n",
    "\n",
    "# Predict using ensemble\n",
    "y_true, y_probs, y_preds = ensemble_predict(models, test_loader, device=device)\n",
    "\n",
    "# Evaluate and visualize\n",
    "evaluate_ensemble(y_true, y_probs, y_preds)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
